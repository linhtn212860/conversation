#!/usr/bin/env python3
"""
=============================================================================
CHAT OPTIMIZED - Phi√™n b·∫£n t·ªëi ∆∞u h√≥a cho Qwen3-8B
=============================================================================

T√°c gi·∫£: ƒê∆∞·ª£c t·∫°o b·ªüi AI Assistant
M·ª•c ƒë√≠ch: Chatbot th√¥ng minh v·ªõi kh·∫£ nƒÉng:
    - Streaming output (hi·ªÉn th·ªã t·ª´ng token)
    - 4-bit quantization (gi·∫£m b·ªô nh·ªõ, tƒÉng t·ªëc)
    - Tra c·ª©u web th·ªùi gian th·ª±c
    - H·ªó tr·ª£ song ng·ªØ Vi·ªát-Anh

Y√™u c·∫ßu:
    - Python 3.8+
    - transformers
    - torch
    - bitsandbytes (cho 4-bit quantization)
    - ddgs ho·∫∑c duckduckgo-search (cho web search)
=============================================================================
"""

# =============================================================================
# PH·∫¶N 1: IMPORT C√ÅC TH∆Ø VI·ªÜN
# =============================================================================

import torch                    # PyTorch - framework deep learning
import re                       # Regular expressions - x·ª≠ l√Ω pattern matching
from pathlib import Path        # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n file cross-platform
from datetime import datetime   # L·∫•y ng√†y gi·ªù hi·ªán t·∫°i
from threading import Thread    # Threading - ch·∫°y song song cho streaming

# Import t·ª´ th∆∞ vi·ªán Transformers c·ªßa HuggingFace
from transformers import (
    AutoModelForCausalLM,      # T·ª± ƒë·ªông load model language modeling
    AutoTokenizer,              # T·ª± ƒë·ªông load tokenizer
    TextIteratorStreamer,       # Streamer ƒë·ªÉ yield t·ª´ng token
    BitsAndBytesConfig          # C·∫•u h√¨nh quantization
)

# =============================================================================
# PH·∫¶N 2: IMPORT WEB SEARCH (T√ôY CH·ªåN)
# =============================================================================
# Th·ª≠ import ddgs (phi√™n b·∫£n m·ªõi) tr∆∞·ªõc, n·∫øu kh√¥ng c√≥ th√¨ d√πng duckduckgo_search
try:
    from ddgs import DDGS
    WEB_SEARCH_AVAILABLE = True
except ImportError:
    try:
        from duckduckgo_search import DDGS
        WEB_SEARCH_AVAILABLE = True
    except ImportError:
        WEB_SEARCH_AVAILABLE = False
        print("‚ö†Ô∏è ƒê·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng tra c·ª©u web, h√£y c√†i ƒë·∫∑t: pip install ddgs")

# =============================================================================
# PH·∫¶N 3: C·∫§U H√åNH V√Ä H·∫∞NG S·ªê
# =============================================================================

# ƒê∆∞·ªùng d·∫´n t·ªõi model ƒë√£ download v·ªÅ local
# __file__ = ƒë∆∞·ªùng d·∫´n c·ªßa file Python hi·ªán t·∫°i
# parent = th∆∞ m·ª•c ch·ª©a file
MODEL_PATH = Path(__file__).parent / "models" / "Qwen_Qwen3-8B"

# L·∫•y ng√†y hi·ªán t·∫°i theo ƒë·ªãnh d·∫°ng DD/MM/YYYY
CURRENT_DATE = datetime.now().strftime("%d/%m/%Y")

# =============================================================================
# PH·∫¶N 4: SYSTEM PROMPT (SONG NG·ªÆ + SMART SEARCH)
# =============================================================================
# System prompt ƒë·ªãnh nghƒ©a "nh√¢n c√°ch" v√† quy t·∫Øc h√†nh x·ª≠ c·ªßa chatbot
# Bao g·ªìm h∆∞·ªõng d·∫´n ƒë·ªÉ model T·ª∞ QUY·∫æT ƒê·ªäNH khi n√†o c·∫ßn search

SYSTEM_PROMPT = f"""You are a friendly home robot assistant having a casual conversation. Today is {CURRENT_DATE}.

WHEN TO SEARCH:
If you need current info you don't know, start with: [SEARCH: query]

HOW TO RESPOND:
- Talk like you're chatting with a friend, not writing an article
- NO markdown formatting (no ###, no **, no bullet points)
- Just speak naturally in paragraphs
- Keep it conversational, warm, and friendly
- Use emoji sometimes üòä
- If sharing news/info, summarize it naturally like telling a story
- DON'T make up facts - use [SEARCH:] if unsure

Match the user's language (Vietnamese or English).

---

B·∫°n l√† robot nh√† ƒëang tr√≤ chuy·ªán th√¢n thi·ªán. H√¥m nay l√† {CURRENT_DATE}.

C√ÅCH TR·∫¢ L·ªúI:
- N√≥i nh∆∞ ƒëang chat v·ªõi b·∫°n b√®, kh√¥ng ph·∫£i vi·∫øt b√†i
- KH√îNG d√πng markdown (kh√¥ng ###, kh√¥ng **, kh√¥ng g·∫°ch ƒë·∫ßu d√≤ng)
- N√≥i t·ª± nhi√™n theo ƒëo·∫°n vƒÉn
- Th√¢n thi·ªán, g·∫ßn g≈©i, ·∫•m √°p
- Th·ªânh tho·∫£ng d√πng emoji üòä
- Khi chia s·∫ª tin t·ª©c, k·ªÉ l·∫°i t·ª± nhi√™n nh∆∞ ƒëang k·ªÉ chuy·ªán
- Kh√¥ng b·ªãa th√¥ng tin - d√πng [SEARCH:] n·∫øu kh√¥ng ch·∫Øc"""


# =============================================================================
# PH·∫¶N 5: H√ÄM LOAD MODEL
# =============================================================================

def load_model(use_4bit: bool = True):
    """
    Load model Qwen3-8B v·ªõi t√πy ch·ªçn quantization.
    
    4-bit Quantization l√† g√¨?
    -------------------------
    - B√¨nh th∆∞·ªùng, m·ªói tham s·ªë model ƒë∆∞·ª£c l∆∞u d·∫°ng float32 (32 bit) ho·∫∑c float16 (16 bit)
    - 4-bit quantization n√©n xu·ªëng c√≤n 4 bit/tham s·ªë
    - Gi·∫£m ~75% b·ªô nh·ªõ GPU c·∫ßn thi·∫øt
    - TƒÉng t·ªëc inference v√¨ √≠t data ph·∫£i di chuy·ªÉn
    - Ch·∫•t l∆∞·ª£ng output gi·∫£m nh·∫π nh∆∞ng v·∫´n ch·∫•p nh·∫≠n ƒë∆∞·ª£c
    
    Args:
        use_4bit (bool): True = d√πng 4-bit quantization, False = d√πng bfloat16
    
    Returns:
        tuple: (model, tokenizer)
    """
    print(f"üîÑ ƒêang t·∫£i model t·ª´ {MODEL_PATH}...")
    
    # -----------------------------------------------------
    # B∆Ø·ªöC 1: Load Tokenizer
    # -----------------------------------------------------
    # Tokenizer chuy·ªÉn ƒë·ªïi text <-> tokens (s·ªë nguy√™n)
    # V√≠ d·ª•: "Xin ch√†o" -> [123, 456, 789]
    tokenizer = AutoTokenizer.from_pretrained(
        str(MODEL_PATH),           # ƒê∆∞·ªùng d·∫´n model
        trust_remote_code=True,    # Cho ph√©p ch·∫°y code t·ª´ model (c·∫ßn cho Qwen)
        local_files_only=True      # Ch·ªâ load t·ª´ local, kh√¥ng download
    )
    
    # -----------------------------------------------------
    # B∆Ø·ªöC 2: Load Model v·ªõi Quantization (n·∫øu c√≥)
    # -----------------------------------------------------
    if use_4bit:
        print("üì¶ S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ tƒÉng t·ªëc...")
        
        # C·∫•u h√¨nh BitsAndBytes cho 4-bit
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,                    # B·∫≠t 4-bit
            bnb_4bit_compute_dtype=torch.bfloat16, # Dtype khi t√≠nh to√°n
            bnb_4bit_use_double_quant=True,       # Double quantization (ti·∫øt ki·ªám th√™m)
            bnb_4bit_quant_type="nf4"             # Lo·∫°i quantization: NormalFloat4
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            str(MODEL_PATH),
            quantization_config=quantization_config,  # √Åp d·ª•ng c·∫•u h√¨nh quant
            device_map="auto",                        # T·ª± ƒë·ªông ph√¢n b·ªï GPU/CPU
            trust_remote_code=True,
            local_files_only=True
        )
    else:
        # Kh√¥ng d√πng quantization - load model v·ªõi bfloat16
        # bfloat16 = Brain Floating Point 16-bit (t·ªëi ∆∞u cho AI)
        model = AutoModelForCausalLM.from_pretrained(
            str(MODEL_PATH),
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True
        )
    
    print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\n")
    return model, tokenizer


# =============================================================================
# PH·∫¶N 6: C√ÅC H√ÄM T√åM KI·∫æM WEB
# =============================================================================

def web_search(query: str, max_results: int = 3) -> str:
    """
    T√¨m ki·∫øm th√¥ng tin tr√™n web b·∫±ng DuckDuckGo.
    
    DuckDuckGo ƒë∆∞·ª£c ch·ªçn v√¨:
    - Mi·ªÖn ph√≠, kh√¥ng c·∫ßn API key
    - Kh√¥ng theo d√µi ng∆∞·ªùi d√πng
    - H·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët
    
    Args:
        query (str): T·ª´ kh√≥a t√¨m ki·∫øm
        max_results (int): S·ªë k·∫øt qu·∫£ t·ªëi ƒëa (gi·∫£m = nhanh h∆°n)
    
    Returns:
        str: K·∫øt qu·∫£ ƒë√£ format
    """
    if not WEB_SEARCH_AVAILABLE:
        return "‚ùå Web search not available."
    
    try:
        print(f"üîç Searching: {query}")
        
        # DDGS = DuckDuckGo Search
        # S·ª≠ d·ª•ng context manager (with) ƒë·ªÉ t·ª± ƒë·ªông cleanup
        with DDGS() as ddgs:
            # ddgs.text() = t√¨m ki·∫øm vƒÉn b·∫£n
            # region='vn-vi' = ∆∞u ti√™n k·∫øt qu·∫£ ti·∫øng Vi·ªát t·ª´ Vi·ªát Nam
            results = list(ddgs.text(query, region='vn-vi', max_results=max_results))
        
        if not results:
            return f"No results for: {query}"
        
        # Format k·∫øt qu·∫£ d·∫°ng danh s√°ch
        formatted = f"Search results for '{query}':\n\n"
        for i, r in enumerate(results, 1):
            # M·ªói k·∫øt qu·∫£ c√≥ title v√† body (m√¥ t·∫£)
            formatted += f"{i}. {r.get('title', '')}: {r.get('body', '')}\n"
        
        return formatted
    except Exception as e:
        return f"Search error: {e}"


def search_news(topic: str, max_results: int = 3) -> str:
    """
    T√¨m tin t·ª©c m·ªõi nh·∫•t v·ªÅ m·ªôt ch·ªß ƒë·ªÅ.
    
    Kh√°c v·ªõi web_search, h√†m n√†y d√πng ddgs.news() ƒë·ªÉ l·∫•y tin t·ª©c
    c√≥ ngu·ªìn (source) v√† th·ªùi gian c·ª• th·ªÉ.
    """
    if not WEB_SEARCH_AVAILABLE:
        return "‚ùå Web search not available."
    
    try:
        print(f"üì∞ Searching news: {topic}")
        with DDGS() as ddgs:
            results = list(ddgs.news(topic, region='vn-vi', max_results=max_results))
        
        if not results:
            return f"No news for: {topic}"
        
        formatted = f"Latest news about '{topic}':\n\n"
        for i, r in enumerate(results, 1):
            formatted += f"{i}. [{r.get('source', '')}] {r.get('title', '')}\n"
        
        return formatted
    except Exception as e:
        return f"News search error: {e}"


# =============================================================================
# PH·∫¶N 7: POST-PROCESSING - X√ìA MARKDOWN
# =============================================================================

def clean_response(text: str) -> str:
    """
    X√≥a markdown formatting ƒë·ªÉ response t·ª± nhi√™n nh∆∞ n√≥i chuy·ªán.
    
    X·ª≠ l√Ω:
    - X√≥a ### headers
    - X√≥a ** bold **
    - X√≥a - bullet points
    - X√≥a --- horizontal rules
    - X√≥a numbered lists (1. 2. 3.)
    """
    import re
    
    # X√≥a markdown headers (### text)
    text = re.sub(r'^#{1,6}\s*', '', text, flags=re.MULTILINE)
    
    # X√≥a bold markers (**text** ho·∫∑c __text__)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'__([^_]+)__', r'\1', text)
    
    # X√≥a italic markers (*text* ho·∫∑c _text_)
    text = re.sub(r'(?<!\*)\*([^*]+)\*(?!\*)', r'\1', text)
    
    # X√≥a horizontal rules (---)
    text = re.sub(r'^-{3,}$', '', text, flags=re.MULTILINE)
    
    # X√≥a bullet points (- text)
    text = re.sub(r'^\s*[-*]\s+', '', text, flags=re.MULTILINE)
    
    # X√≥a numbered lists nh∆∞ng gi·ªØ n·ªôi dung (1. text -> text)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    
    # X√≥a d√≤ng tr·ªëng th·ª´a (nhi·ªÅu d√≤ng tr·ªëng -> 1 d√≤ng)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # Strip whitespace
    text = text.strip()
    
    return text


# =============================================================================
# PH·∫¶N 8: PH√ÅT HI·ªÜN √ù ƒê·ªäNH T√åM KI·∫æM
# =============================================================================

def detect_search_intent(user_input: str) -> tuple:
    """
    Ph√¢n t√≠ch c√¢u h·ªèi c·ªßa user ƒë·ªÉ x√°c ƒë·ªãnh c√≥ c·∫ßn t√¨m ki·∫øm web kh√¥ng.
    
    Ho·∫°t ƒë·ªông:
    1. Chuy·ªÉn input v·ªÅ ch·ªØ th∆∞·ªùng
    2. So s√°nh v·ªõi c√°c t·ª´ kh√≥a ƒë√£ ƒë·ªãnh nghƒ©a
    3. N·∫øu match -> c·∫ßn t√¨m ki·∫øm
    
    Returns:
        tuple: (needs_search: bool, search_type: str, search_query: str)
    """
    user_lower = user_input.lower()
    
    # Dictionary ch·ª©a c√°c lo·∫°i search v√† t·ª´ kh√≥a t∆∞∆°ng ·ª©ng
    # QUAN TR·ªåNG: Th√™m nhi·ªÅu t·ª´ kh√≥a ƒë·ªÉ tr√°nh model hallucinate (b·ªãa th√¥ng tin)
    keywords = {
        'weather': ['th·ªùi ti·∫øt', 'weather', 'nhi·ªát ƒë·ªô', 'm∆∞a', 'n·∫Øng', 'forecast'],
        'news': ['tin t·ª©c', 'tin m·ªõi', 'th·ªùi s·ª±', 'news', 'update', 'c·∫≠p nh·∫≠t',
                 'ch√≠nh tr·ªã', 'politics', 'politic', 'political',
                 'kinh t·∫ø', 'economy', 'x√£ h·ªôi', 'social',
                 's·ª± ki·ªán', 'event', 'm·ªõi nh·∫•t', 'latest'],
        'price': ['gi√° v√†ng', 'gi√° xƒÉng', 't·ª∑ gi√°', 'bitcoin', 'ch·ª©ng kho√°n', 
                  'stock', 'crypto', 'price'],
        'sports': ['b√≥ng ƒë√°', 'football', 'k·∫øt qu·∫£', 't·ª∑ s·ªë', 'soccer',
                   'qu·∫£ b√≥ng v√†ng', 'ballon d\'or', 'golden ball',  # Gi·∫£i th∆∞·ªüng b√≥ng ƒë√°
                   'world cup', 'champions league', 'premier league',
                   'gi·∫£i th∆∞·ªüng', 'award', 'gi√†nh gi·∫£i', 'champion', 'winner',
                   'oscar', 'grammy', 'nobel',  # C√°c gi·∫£i th∆∞·ªüng kh√°c
                   'v√¥ ƒë·ªãch', 'championship', 'tournament']
    }
    
    for search_type, kws in keywords.items():
        if any(kw in user_lower for kw in kws):
            # Kh√¥ng th√™m ng√†y th√°ng v√†o query - l√†m search kh√¥ng ra k·∫øt qu·∫£
            return True, search_type, user_input
    
    return False, '', ''


# =============================================================================
# PH·∫¶N 8: STREAMING GENERATION (QUAN TR·ªåNG!)
# =============================================================================

def generate_streaming(model, tokenizer, messages: list, max_new_tokens: int = 1024):
    """
    Generate response v·ªõi streaming output - T√çNH NƒÇNG T·ªêI ∆ØU CH√çNH!
    
    STREAMING L√Ä G√å?
    -----------------
    B√¨nh th∆∞·ªùng: Model generate xong TO√ÄN B·ªò c√¢u tr·∫£ l·ªùi -> hi·ªÉn th·ªã
    Streaming: Model generate t·ª´ng TOKEN -> hi·ªÉn th·ªã ngay -> ti·∫øp t·ª•c
    
    L·ª£i √≠ch:
    - User th·∫•y response ngay l·∫≠p t·ª©c (kh√¥ng ph·∫£i ch·ªù)
    - C·∫£m gi√°c nhanh h∆°n nhi·ªÅu
    - C√≥ th·ªÉ Ctrl+C d·ª´ng gi·ªØa ch·ª´ng
    
    C√ÅCH HO·∫†T ƒê·ªòNG:
    ---------------
    1. Main thread: ƒê·ªçc tokens t·ª´ streamer v√† yield ra ngo√†i
    2. Background thread: Ch·∫°y model.generate() 
    3. TextIteratorStreamer: C·∫ßu n·ªëi gi·ªØa 2 threads
    
    Yields:
        str: T·ª´ng ƒëo·∫°n text nh·ªè (c√≥ th·ªÉ l√† 1 t·ª´ ho·∫∑c v√†i k√Ω t·ª±)
    """
    
    # ---------------------------------------------------------
    # B∆Ø·ªöC 1: Chu·∫©n b·ªã input v·ªõi Chat Template
    # ---------------------------------------------------------
    # Chat template = format chu·∫©n ƒë·ªÉ model hi·ªÉu vai tr√≤ c·ªßa m·ªói message
    # V√≠ d·ª• Qwen format:
    # <|im_start|>system
    # You are a helpful assistant.<|im_end|>
    # <|im_start|>user
    # Hello<|im_end|>
    # <|im_start|>assistant
    
    text = tokenizer.apply_chat_template(
        messages,                    # List c√°c message
        tokenize=False,              # Tr·∫£ v·ªÅ string, kh√¥ng tokenize
        add_generation_prompt=True,  # Th√™m prompt cho assistant
        enable_thinking=False        # T·∫Øt thinking mode c·ªßa Qwen3
    )
    
    # Tokenize string th√†nh tensor
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # ---------------------------------------------------------
    # B∆Ø·ªöC 2: T·∫°o TextIteratorStreamer
    # ---------------------------------------------------------
    # Streamer n√†y ho·∫°t ƒë·ªông nh∆∞ m·ªôt queue:
    # - model.generate() push tokens v√†o
    # - V√≤ng for b√™n ngo√†i pop tokens ra
    streamer = TextIteratorStreamer(
        tokenizer, 
        skip_prompt=True,           # B·ªè qua ph·∫ßn prompt, ch·ªâ l·∫•y response
        skip_special_tokens=True    # B·ªè c√°c token ƒë·∫∑c bi·ªát (<|im_end|>, etc.)
    )
    
    # ---------------------------------------------------------
    # B∆Ø·ªöC 3: C·∫•u h√¨nh generation
    # ---------------------------------------------------------
    generation_kwargs = dict(
        **inputs,                           # Input tokens
        max_new_tokens=max_new_tokens,      # Gi·ªõi h·∫°n ƒë·ªô d√†i output
        do_sample=True,                     # B·∫≠t sampling (kh√¥ng greedy)
        temperature=0.7,                    # ƒê·ªô "s√°ng t·∫°o" (0=deterministic, 1=random)
        top_p=0.9,                          # Nucleus sampling
        pad_token_id=tokenizer.eos_token_id,
        streamer=streamer                   # G·∫Øn streamer v√†o
    )
    
    # ---------------------------------------------------------
    # B∆Ø·ªöC 4: Ch·∫°y generation trong thread ri√™ng
    # ---------------------------------------------------------
    # T·∫°i sao c·∫ßn thread? 
    # V√¨ model.generate() l√† blocking - n√≥ ch·∫°y ƒë·∫øn khi xong m·ªõi return
    # Nh∆∞ng ta mu·ªën ƒë·ªçc tokens TRONG KHI n√≥ ƒëang generate
    # -> Gi·∫£i ph√°p: Ch·∫°y generate() ·ªü thread kh√°c
    
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # ---------------------------------------------------------
    # B∆Ø·ªöC 5: Yield tokens khi nh·∫≠n ƒë∆∞·ª£c
    # ---------------------------------------------------------
    # streamer l√† iterator - m·ªói l·∫ßn next() s·∫Ω block cho ƒë·∫øn khi
    # c√≥ token m·ªõi ho·∫∑c generation k·∫øt th√∫c
    
    response_text = ""
    for new_text in streamer:
        response_text += new_text
        yield new_text  # Yield ra ngo√†i ƒë·ªÉ in ngay l·∫≠p t·ª©c
    
    # ƒê·ª£i thread k·∫øt th√∫c
    thread.join()


# =============================================================================
# PH·∫¶N 9: H√ÄM CHAT CH√çNH
# =============================================================================

def chat_with_streaming(model, tokenizer, messages: list, user_input: str):
    """
    X·ª≠ l√Ω chat v·ªõi streaming v√† web search.
    
    Flow M·ªöI (Smart Search):
    1. Ki·ªÉm tra t·ª´ kh√≥a (c√°ch c≈©, v·∫´n gi·ªØ)
    2. N·∫øu kh√¥ng match t·ª´ kh√≥a -> generate response
    3. Ki·ªÉm tra n·∫øu response ch·ª©a [SEARCH: query] -> model t·ª± y√™u c·∫ßu search
    4. Th·ª±c hi·ªán search v√† regenerate v·ªõi k·∫øt qu·∫£
    """
    
    # ---------------------------------------------------------
    # B∆Ø·ªöC 1: Ki·ªÉm tra t·ª´ kh√≥a (Keyword-based detection)
    # ---------------------------------------------------------
    needs_search, search_type, search_query = detect_search_intent(user_input)
    
    if needs_search and WEB_SEARCH_AVAILABLE:
        print(f"üîç Detected search intent (keyword): {search_type}")
        
        if search_type == 'news':
            search_results = search_news(search_query)
        else:
            search_results = web_search(search_query)
        
        augmented_messages = messages.copy()
        augmented_messages[-1] = {
            "role": "user",
            "content": f"{user_input}\n\n[SEARCH RESULTS]:\n{search_results}\n\nBased on this, answer in detail."
        }
        messages_to_use = augmented_messages
        
        # In streaming response v·ªõi search results
        print("\nü§ñ Robot: ", end="", flush=True)
        response = ""
        for token in generate_streaming(model, tokenizer, messages_to_use):
            print(token, end="", flush=True)
            response += token
    else:
        # ---------------------------------------------------------
        # B∆Ø·ªöC 2: Generate response KH√îNG streaming tr∆∞·ªõc ƒë·ªÉ check [SEARCH:]
        # ---------------------------------------------------------
        print("\nü§ñ Robot: ", end="", flush=True)
        
        # Collect response tr∆∞·ªõc (v·∫´n streaming)
        response = ""
        collected_tokens = []
        for token in generate_streaming(model, tokenizer, messages):
            collected_tokens.append(token)
            response += token
        
        # ---------------------------------------------------------
        # B∆Ø·ªöC 3: Ki·ªÉm tra n·∫øu model y√™u c·∫ßu search
        # ---------------------------------------------------------
        search_pattern = r'\[SEARCH:\s*([^\]]+)\]'
        match = re.search(search_pattern, response)
        
        if match and WEB_SEARCH_AVAILABLE:
            # Model y√™u c·∫ßu search - KH√îNG in response c≈©
            model_search_query = match.group(1).strip()
            print(f"ƒêang t√¨m ki·∫øm: {model_search_query}...")
            
            # Th·ª±c hi·ªán search
            search_results = web_search(model_search_query)
            
            # T·∫°o messages m·ªõi v·ªõi k·∫øt qu·∫£ search
            search_messages = messages.copy()
            search_messages.append({
                "role": "assistant",
                "content": f"I need to search for information about: {model_search_query}"
            })
            search_messages.append({
                "role": "user",
                "content": f"[SEARCH RESULTS]:\n{search_results}\n\nNow answer the original question based on these search results. Provide detailed information."
            })
            
            # Regenerate response v·ªõi streaming
            print("\nü§ñ Robot: ", end="", flush=True)
            response = ""
            for token in generate_streaming(model, tokenizer, search_messages):
                print(token, end="", flush=True)
                response += token
        else:
            # Kh√¥ng c·∫ßn search - in ra c√°c tokens ƒë√£ collect
            for token in collected_tokens:
                print(token, end="", flush=True)
    
    print("\n")
    
    # Post-process: x√≥a markdown formatting ƒë·ªÉ response t·ª± nhi√™n h∆°n
    cleaned = clean_response(response.strip())
    return cleaned


# =============================================================================
# PH·∫¶N 10: MAIN - V√íNG L·∫∂P CHAT CH√çNH
# =============================================================================

def main():
    """
    Entry point c·ªßa ch∆∞∆°ng tr√¨nh.
    
    Flow:
    1. H·ªèi user c√≥ mu·ªën d√πng 4-bit kh√¥ng
    2. Load model
    3. V√≤ng l·∫∑p chat v√¥ h·∫°n
    4. X·ª≠ l√Ω c√°c l·ªánh ƒë·∫∑c bi·ªát
    5. Generate v√† hi·ªÉn th·ªã response
    """
    
    # Header
    print("=" * 60)
    print("ü§ñ QWEN3-8B OPTIMIZED CHAT")
    print("=" * 60)
    
    # H·ªèi v·ªÅ quantization
    use_4bit = input("S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ tƒÉng t·ªëc? (y/n, default=y): ").strip().lower()
    use_4bit = use_4bit != 'n'  # M·∫∑c ƒë·ªãnh l√† Yes
    
    # Load model
    model, tokenizer = load_model(use_4bit=use_4bit)
    
    # Kh·ªüi t·∫°o conversation history
    # B·∫Øt ƒë·∫ßu v·ªõi system prompt
    conversation_history = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
    
    # Hi·ªÉn th·ªã th√¥ng tin
    print("=" * 60)
    print(f"üìÖ Today: {CURRENT_DATE}")
    print(f"üåê Web search: {'‚úÖ Enabled' if WEB_SEARCH_AVAILABLE else '‚ùå Disabled'}")
    print(f"‚ö° 4-bit quantization: {'‚úÖ Enabled' if use_4bit else '‚ùå Disabled'}")
    print("=" * 60)
    print("Commands: 'quit', 'clear', 'history', 'news', 'search: <query>'")
    print("=" * 60 + "\n")
    
    # V√≤ng l·∫∑p chat ch√≠nh
    while True:
        try:
            # Nh·∫≠n input t·ª´ user
            user_input = input("üë§ You: ").strip()
            
            if not user_input:
                continue
            
            # -----------------------------------------
            # X·ª≠ l√Ω c√°c l·ªánh ƒë·∫∑c bi·ªát
            # -----------------------------------------
            if user_input.lower() in ['quit', 'exit', 'tho√°t']:
                print("\nüëã Goodbye!")
                break
            
            if user_input.lower() == 'clear':
                # Reset history v·ªÅ ch·ªâ c√≤n system prompt
                conversation_history = [{"role": "system", "content": SYSTEM_PROMPT}]
                print("üóëÔ∏è History cleared.\n")
                continue
            
            if user_input.lower() == 'history':
                # Hi·ªÉn th·ªã l·ªãch s·ª≠ (b·ªè qua system prompt)
                print("\nüìú CONVERSATION HISTORY:")
                for msg in conversation_history[1:]:
                    role = "üë§" if msg["role"] == "user" else "ü§ñ"
                    # Hi·ªÉn th·ªã t·ªëi ƒëa 100 k√Ω t·ª±
                    print(f"{role}: {msg['content'][:100]}...")
                print()
                continue
            
            if user_input.lower().startswith('search:'):
                # T√¨m ki·∫øm th·ªß c√¥ng
                query = user_input[7:].strip()
                print(web_search(query))
                continue
            
            if user_input.lower() == 'news':
                # Xem tin t·ª©c nhanh
                print(search_news("Vietnam"))
                continue
            
            # -----------------------------------------
            # Chat th√¥ng th∆∞·ªùng
            # -----------------------------------------
            
            # Th√™m message c·ªßa user v√†o history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Generate response v·ªõi streaming
            response = chat_with_streaming(model, tokenizer, conversation_history, user_input)
            
            # Th√™m response c·ªßa assistant v√†o history
            conversation_history.append({"role": "assistant", "content": response})
            
        except KeyboardInterrupt:
            # Ctrl+C ƒë·ªÉ tho√°t
            print("\n\nüëã Goodbye!")
            break
        except Exception as e:
            print(f"\n‚ùå Error: {e}\n")
            continue


# =============================================================================
# ENTRY POINT
# =============================================================================
# Ch·ªâ ch·∫°y main() khi file ƒë∆∞·ª£c th·ª±c thi tr·ª±c ti·∫øp
# Kh√¥ng ch·∫°y khi file ƒë∆∞·ª£c import nh∆∞ module

if __name__ == "__main__":
    main()
